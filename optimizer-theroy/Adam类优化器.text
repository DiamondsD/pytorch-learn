  Adam (Kingma and Ba, 2014) 是另一种学习率自适应的优化算法。Adam结合了动量和RMSprop的思想，通过计算梯度的一阶矩和二阶矩，并进行偏差矫正 
  动量机制：跟踪梯度的一阶矩（均值），加速梯度方向一致的参数更新。 动量（t+1） = x*动量（t）+ （1-x）*梯度
  自适应学习率：跟踪梯度的二阶矩（方差），为每个参数分配独立的学习率，抑制梯度变化大的参数更新幅度。
  偏差校正：修正初始时刻矩估计的偏差，确保迭代初期数值稳定。
  
  自我解释：
  相当于学习来看乘上 一阶矩除以二阶矩的次方根（还要加上一个极小值防止分母为0），
  这里的一阶矩和二阶矩都是加上了偏差矫正的，防止初始阶段的零初始化偏差，
  矫正方式是除以1-x的t次方，这里的x是指指数衰减率，通常取0.9，随着时间的增加变得越来越接近于1，分母就越来越大



AdamW优化器：AdamW是属于Adam的变体，他核心的改进点，是解耦了权重衰减，将他和梯度更新分开，直接作用于参数本身而不再混入梯度计算，
因为adam由SGD演变而来，在SGD中权重衰减系数等价于正则化系数，即减去学习率乘以当前梯度，但是在adam算法中，梯度计算包含自适应学习率，
导致正则化的梯度也会被自适应调整，例如除以梯度的二阶矩估计，这就破坏了权重衰减的等价性，
即每次的衰减系数会因为更新次数变化，使得正则化效果不稳定
在adamW中，直接减去初始学习率乘以当前梯度，与自适应学习率分开.
