BGD算法：基本梯度下降优化算法BGD每次参数更新时会使用在原来权重的基础上减去整个训练数据集的梯度。
好处：1，在理想情况下，有潜力收敛到全局最优解，但也有可能陷入局部最优解。2，因为使用的是整个数据集的梯度，所以稳定性比较高，参数更新时不容易受到单个样本或噪声的影响
坏处： 效率低，因为使用整个数据集的梯度更新，每次更新时要把所有梯度计算出来，在大模型上使用时会让训练时间变得很长

SGD算法：中文名随机梯度下降，是一种基于随机采样的梯度下降优化算法，与BGD不同，SGD每次更新只使用单个或小批量样本的梯度进行参数更新
好处：1.更新的速度很快，这在大模型上很有优势，尤其是在迭代次数较少的情况下。2.因为SGD随机采样和快速更新的特点，SGD可以在多个局部最小值之间搜索，找到更好的局部最小值和接近全局最优解
坏处：1.因为其更新的随机性，容易陷入局部最小值，也因为其随机性也有助于跳出最小值，寻找更好的解。

Mini-BGD :小批量梯度下降，介于BGD和SGD之间，与BGD相比速度更快，与SGD相比更新要慢一点但是更稳定，而且梯度估计的抖动减少

SGD with momentum ：带动量的SGD